{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pandas ...\n",
      "Installing openpyxl ...\n",
      "Installing requests ...\n",
      "\n",
      "Installation completed.\n",
      "All packages installed successfully.\n",
      "Elapsed time: 6.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Upgrade pip first\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error upgrading pip:\", e)\n",
    "\n",
    "# Required packages for this code snippet\n",
    "packages = [\"pandas\", \"openpyxl\", \"requests\"]\n",
    "\n",
    "errors = []\n",
    "start_time = time.time()\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        print(f\"Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        errors.append(f\"{pkg}: {e}\")\n",
    "        print(f\"Failed to install {pkg}. Continuing.\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\nInstallation completed.\")\n",
    "if errors:\n",
    "    print(\"Encountered the following errors:\")\n",
    "    for err in errors:\n",
    "        print(\" -\", err)\n",
    "else:\n",
    "    print(\"All packages installed successfully.\")\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ticker: AAPL, CIK: 320193, Year: 2023\n",
      "Structured JSON saved to: sec_10k_data\\AAPL_2023_10K_graph.json\n",
      "Script executed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def install_if_needed(package):\n",
    "    \"\"\"Utility function to ensure a package is installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package])\n",
    "\n",
    "# Ensure required packages for your environment\n",
    "for pkg in [\"pandas\", \"openpyxl\", \"requests\", \"beautifulsoup4\", \"tqdm\"]:\n",
    "    install_if_needed(pkg)\n",
    "\n",
    "# Headers for requests (SEC recommends a User-Agent with your email)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"RAGFinancialReport/1.0 (your_email@example.com)\"\n",
    "}\n",
    "\n",
    "# Set TEST_MODE = True to only process one ticker for one year (2023)\n",
    "TEST_MODE = True\n",
    "\n",
    "def get_all_tickers_info():\n",
    "    \"\"\"\n",
    "    Fetch the SEC-provided JSON mapping of all known tickers to their CIKs.\n",
    "    Returns a dict of the form:\n",
    "      { '0': {'cik_str': 320193, 'ticker': 'AAPL', 'title': 'Apple Inc.'}, ... }\n",
    "    \"\"\"\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching tickers info:\", e)\n",
    "        return {}\n",
    "\n",
    "def find_cik(ticker, tickers_data):\n",
    "    \"\"\"\n",
    "    Look up the CIK for a given ticker using the data from get_all_tickers_info().\n",
    "    Returns the CIK (as an integer) if found, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for _, info in tickers_data.items():\n",
    "            if info[\"ticker\"].lower() == ticker.lower():\n",
    "                return info[\"cik_str\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error finding CIK for\", ticker, \":\", e)\n",
    "    return None\n",
    "\n",
    "def extract_text_from_ixbrl(html_soup):\n",
    "    \"\"\"\n",
    "    Extract text from Inline XBRL <ix:nonNumeric> and <ix:nonFraction> tags.\n",
    "    Return a concatenated string of all text found.\n",
    "    \"\"\"\n",
    "    text_segments = []\n",
    "\n",
    "    # Gather text from <ix:nonNumeric> tags\n",
    "    non_numeric_tags = html_soup.find_all(lambda tag: tag.name and \"nonNumeric\" in tag.name)\n",
    "    for tag in non_numeric_tags:\n",
    "        # Some tags might contain child tags or might be empty\n",
    "        txt = tag.get_text(strip=True)\n",
    "        if txt:\n",
    "            text_segments.append(txt)\n",
    "\n",
    "    # Gather text from <ix:nonFraction> tags\n",
    "    non_fraction_tags = html_soup.find_all(lambda tag: tag.name and \"nonFraction\" in tag.name)\n",
    "    for tag in non_fraction_tags:\n",
    "        txt = tag.get_text(strip=True)\n",
    "        if txt:\n",
    "            text_segments.append(txt)\n",
    "\n",
    "    # Join everything with a couple newlines\n",
    "    return \"\\n\\n\".join(text_segments)\n",
    "\n",
    "def is_ixbrl_document(html_text):\n",
    "    \"\"\"\n",
    "    Heuristic check: If we see <ix: in the HTML text, it's likely an Inline XBRL doc.\n",
    "    \"\"\"\n",
    "    return \"<ix:\" in html_text or \"<ix:\" in html_text.lower()\n",
    "\n",
    "def fetch_10k_text_and_link_from_index(index_url):\n",
    "    \"\"\"\n",
    "    Given the '-index.htm' URL for a particular filing:\n",
    "      1) Parse the table (tableFile) to find the row where Type == '10-K' or '10-K/A'.\n",
    "      2) Return the actual 10-K document's URL and download its text (handle iXBRL).\n",
    "      3) Optionally, gather 'GRAPHIC' documents.\n",
    "    \n",
    "    Returns a dict:\n",
    "    {\n",
    "       \"tenk_link\": <URL to the actual 10-K doc>,\n",
    "       \"tenk_text\": <the full text extracted>,\n",
    "       \"graphics\": [\n",
    "          {\"filename\": \"...\", \"graphic_url\": \"...\", \"local_path\": \"\"},\n",
    "          ...\n",
    "       ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"tenk_link\": None,\n",
    "        \"tenk_text\": \"\",\n",
    "        \"graphics\": []\n",
    "    }\n",
    "    try:\n",
    "        # 1) Get the index page\n",
    "        resp = requests.get(index_url, headers=HEADERS, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # 2) Find table rows that contain the docs\n",
    "        rows = soup.select(\"table.tableFile tr\")\n",
    "        tenk_link = None\n",
    "\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) < 4:\n",
    "                continue\n",
    "            # 'Type' is typically in cells[3]\n",
    "            doc_type = cells[3].get_text(strip=True)\n",
    "            link_tag = cells[2].find(\"a\")\n",
    "            if link_tag:\n",
    "                href = link_tag.get(\"href\", \"\")\n",
    "                if href.startswith(\"/\"):\n",
    "                    href = \"https://www.sec.gov\" + href\n",
    "\n",
    "                # Identify the main 10-K document\n",
    "                if doc_type in [\"10-K\", \"10-K/A\"]:\n",
    "                    tenk_link = href\n",
    "\n",
    "                # Identify graphics files\n",
    "                if doc_type == \"GRAPHIC\":\n",
    "                    filename = cells[2].get_text(strip=True)\n",
    "                    result[\"graphics\"].append({\n",
    "                        \"filename\": filename,\n",
    "                        \"graphic_url\": href,\n",
    "                        \"local_path\": \"\"  # to be filled if downloaded\n",
    "                    })\n",
    "\n",
    "        if not tenk_link:\n",
    "            print(f\"Could not find a 10-K doc link on index page: {index_url}\")\n",
    "            return result\n",
    "\n",
    "        # 3) Download the actual 10-K text\n",
    "        r2 = requests.get(tenk_link, headers=HEADERS, timeout=20)\n",
    "        r2.raise_for_status()\n",
    "\n",
    "        # Check if this is an Inline XBRL doc\n",
    "        text_html = r2.text\n",
    "        text_soup = BeautifulSoup(text_html, \"html.parser\")\n",
    "\n",
    "        if is_ixbrl_document(text_html):\n",
    "            # Parse iXBRL tags\n",
    "            full_text = extract_text_from_ixbrl(text_soup)\n",
    "        else:\n",
    "            # Fallback: Just get all text\n",
    "            full_text = text_soup.get_text(separator=\"\\n\")\n",
    "\n",
    "        result[\"tenk_link\"] = tenk_link\n",
    "        result[\"tenk_text\"] = full_text\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching full 10-K text:\", e)\n",
    "        return result\n",
    "\n",
    "def chunk_text_by_paragraphs(text, max_chars=2000):\n",
    "    \"\"\"\n",
    "    Splits text into chunks no larger than max_chars by paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para_len = len(para) + 1  # +1 for newline or spacing\n",
    "        if current_length + para_len > max_chars:\n",
    "            # flush current_chunk\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk = [para]\n",
    "            current_length = para_len\n",
    "        else:\n",
    "            current_chunk.append(para)\n",
    "            current_length += para_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def build_knowledge_graph(chunks):\n",
    "    \"\"\"\n",
    "    Builds a simple knowledge graph from text chunks.\n",
    "    Each chunk becomes a node and nodes are sequentially connected.\n",
    "    \n",
    "    Returns a dict with 'nodes' and 'edges'.\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        node_id = f\"chunk_{i+1}\"\n",
    "        nodes.append({\n",
    "            \"id\": node_id,\n",
    "            \"type\": \"text_chunk\",\n",
    "            \"content\": chunk.strip()\n",
    "        })\n",
    "        if i > 0:\n",
    "            edges.append({\n",
    "                \"source\": f\"chunk_{i}\",\n",
    "                \"target\": node_id,\n",
    "                \"relationship\": \"next\"\n",
    "            })\n",
    "    return {\"nodes\": nodes, \"edges\": edges}\n",
    "\n",
    "def fetch_10k_filings_for_year(cik, year):\n",
    "    \"\"\"\n",
    "    Uses the SEC submissions JSON endpoint for the given CIK,\n",
    "    filters for form == '10-K' (or '10-K/A') with a filing date in the given year,\n",
    "    and fetches the 10-K text and links.\n",
    "    \n",
    "    Returns a list of dicts containing:\n",
    "    {\n",
    "       \"form_type\": ...,\n",
    "       \"filing_date\": ...,\n",
    "       \"accession_no\": ...,\n",
    "       \"doc_index_url\": ...,\n",
    "       \"tenk_link\": ...,\n",
    "       \"tenk_text\": ...,\n",
    "       \"graphics\": [...],\n",
    "    }\n",
    "    \"\"\"\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{str(cik).zfill(10)}.json\"\n",
    "    results = []\n",
    "    try:\n",
    "        r = requests.get(base_url, headers=HEADERS, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to fetch data for CIK {cik} in {year}: status code {r.status_code}\")\n",
    "            return results\n",
    "        data = r.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for CIK {cik} in {year}:\", e)\n",
    "        return results\n",
    "\n",
    "    try:\n",
    "        recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        forms = recent.get(\"form\", [])\n",
    "        dates = recent.get(\"filingDate\", [])\n",
    "        accessions = recent.get(\"accessionNumber\", [])\n",
    "\n",
    "        for i in range(len(forms)):\n",
    "            form_type = forms[i]\n",
    "            filing_date = dates[i]\n",
    "            accession_no = accessions[i]\n",
    "\n",
    "            if form_type in [\"10-K\", \"10-K/A\"] and filing_date.startswith(str(year)):\n",
    "                doc_index_url = (\n",
    "                    f\"https://www.sec.gov/Archives/edgar/data/\"\n",
    "                    f\"{int(cik)}/{accession_no.replace('-', '')}/{accession_no}-index.htm\"\n",
    "                )\n",
    "                tenk_data = fetch_10k_text_and_link_from_index(doc_index_url)\n",
    "                results.append({\n",
    "                    \"form_type\": form_type,\n",
    "                    \"filing_date\": filing_date,\n",
    "                    \"accession_no\": accession_no,\n",
    "                    \"doc_index_url\": doc_index_url,\n",
    "                    \"tenk_link\": tenk_data[\"tenk_link\"],\n",
    "                    \"tenk_text\": tenk_data[\"tenk_text\"],\n",
    "                    \"graphics\": tenk_data[\"graphics\"]\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing filings for CIK {cik} in {year}:\", e)\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # For testing, set a flag to only process one ticker and one year\n",
    "        if TEST_MODE:\n",
    "            ticker = \"AAPL\"  # Example ticker\n",
    "            year = 2023\n",
    "            tickers_data = get_all_tickers_info()\n",
    "            cik = find_cik(ticker, tickers_data)\n",
    "            if not cik:\n",
    "                print(f\"CIK not found for {ticker}\")\n",
    "                return\n",
    "\n",
    "            print(f\"Processing ticker: {ticker}, CIK: {cik}, Year: {year}\")\n",
    "            filings = fetch_10k_filings_for_year(cik, year)\n",
    "            if not filings:\n",
    "                print(f\"No filings found for {ticker} in {year}.\")\n",
    "                return\n",
    "\n",
    "            # Process only the first filing in the list\n",
    "            filing = filings[0]\n",
    "            # Chunk the text\n",
    "            chunks = chunk_text_by_paragraphs(filing[\"tenk_text\"], max_chars=3000)\n",
    "            # Build a simple knowledge graph\n",
    "            kg = build_knowledge_graph(chunks)\n",
    "\n",
    "            # Structure the final JSON\n",
    "            structured_data = {\n",
    "                \"company\": {\n",
    "                    \"ticker\": ticker,\n",
    "                    \"cik\": cik\n",
    "                },\n",
    "                \"filing\": {\n",
    "                    \"year\": year,\n",
    "                    \"form_type\": filing[\"form_type\"],\n",
    "                    \"filing_date\": filing[\"filing_date\"],\n",
    "                    \"accession_no\": filing[\"accession_no\"],\n",
    "                    \"doc_index_url\": filing[\"doc_index_url\"],\n",
    "                    \"tenk_link\": filing[\"tenk_link\"],\n",
    "                    \"tenk_text\": filing[\"tenk_text\"],\n",
    "                    \"graphics\": filing[\"graphics\"],\n",
    "                    \"text_chunks\": [\n",
    "                        {\"chunk_id\": f\"chunk_{i+1}\", \"content\": chunk}\n",
    "                        for i, chunk in enumerate(chunks)\n",
    "                    ],\n",
    "                    \"knowledge_graph\": kg\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Save the structured JSON\n",
    "            output_dir = \"sec_10k_data\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, f\"{ticker}_{year}_10K_graph.json\")\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(structured_data, f, indent=2)\n",
    "            print(f\"Structured JSON saved to: {output_path}\")\n",
    "\n",
    "        else:\n",
    "            # Normal mode: iterate multiple tickers/years from an Excel file, etc.\n",
    "            excel_file = r\"C:\\path\\to\\your\\Data_companies_list.xlsx\"\n",
    "            df = pd.read_excel(excel_file)\n",
    "            tickers_data = get_all_tickers_info()\n",
    "            if not tickers_data:\n",
    "                print(\"No tickers data available. Exiting.\")\n",
    "                return\n",
    "            years = range(2012, 2026)\n",
    "            output_dir = \"sec_10k_data\"\n",
    "            if os.path.exists(output_dir):\n",
    "                shutil.rmtree(output_dir)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                ticker = str(row[\"Symbol\"]).strip()\n",
    "                cik = find_cik(ticker, tickers_data)\n",
    "                if not cik:\n",
    "                    print(f\"CIK not found for {ticker}\")\n",
    "                    continue\n",
    "                ticker_dir = os.path.join(output_dir, ticker)\n",
    "                os.makedirs(ticker_dir, exist_ok=True)\n",
    "\n",
    "                for y in tqdm(years, desc=f\"Fetching 10-K for {ticker}\"):\n",
    "                    filings = fetch_10k_filings_for_year(cik, y)\n",
    "                    if filings:\n",
    "                        for filing in filings:\n",
    "                            chunks = chunk_text_by_paragraphs(filing[\"tenk_text\"], max_chars=3000)\n",
    "                            kg = build_knowledge_graph(chunks)\n",
    "                            structured_data = {\n",
    "                                \"company\": {\"ticker\": ticker, \"cik\": cik},\n",
    "                                \"filing\": {\n",
    "                                    \"year\": y,\n",
    "                                    \"form_type\": filing[\"form_type\"],\n",
    "                                    \"filing_date\": filing[\"filing_date\"],\n",
    "                                    \"accession_no\": filing[\"accession_no\"],\n",
    "                                    \"doc_index_url\": filing[\"doc_index_url\"],\n",
    "                                    \"tenk_link\": filing[\"tenk_link\"],\n",
    "                                    \"tenk_text\": filing[\"tenk_text\"],\n",
    "                                    \"graphics\": filing[\"graphics\"],\n",
    "                                    \"text_chunks\": [\n",
    "                                        {\"chunk_id\": f\"chunk_{i+1}\", \"content\": chunk}\n",
    "                                        for i, chunk in enumerate(chunks)\n",
    "                                    ],\n",
    "                                    \"knowledge_graph\": kg\n",
    "                                }\n",
    "                            }\n",
    "                            # Save per filing\n",
    "                            fname = f\"{ticker}_{y}_{filing['accession_no']}_10K_graph.json\"\n",
    "                            save_path = os.path.join(ticker_dir, fname)\n",
    "                            try:\n",
    "                                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                                    json.dump(structured_data, f, indent=2)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error saving file {save_path}:\", e)\n",
    "                    time.sleep(0.2)\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error in main execution:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Script executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2746 documents.\n",
      "Vector store created successfully.\n",
      "RetrievalQA chain is ready.\n",
      "\n",
      "--- RAG Query System ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "# --- Auto-install required packages ---\n",
    "def install_if_needed(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg_name])\n",
    "\n",
    "# Basic packages for our pipeline\n",
    "for pkg, imp in [(\"langchain\", None), (\"langchain-community\", None),\n",
    "                 (\"langchain-huggingface\", \"langchain_huggingface\"),\n",
    "                 (\"transformers\", None), (\"sentence_transformers\", \"sentence_transformers\"),\n",
    "                 (\"faiss-cpu\", None)]:\n",
    "    install_if_needed(pkg, imp)\n",
    "\n",
    "# --- Import libraries ---\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "# Try importing the new HuggingFacePipeline from langchain_huggingface, fallback if needed.\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "except ImportError:\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import transformers\n",
    "\n",
    "# --- Set up free LLM using HuggingFacePipeline ---\n",
    "model_name = \"google/flan-t5-small\"\n",
    "# Enable sampling to use temperature (even if 0 yields greedy behavior)\n",
    "pipe = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    temperature=0\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --- Set up embeddings for vector store and similarity ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Function to load SEC 10-K documents from JSON files ---\n",
    "def load_documents(data_dir=\"sec_10k_data\"):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        filings = json.load(f)\n",
    "                        for filing in filings:\n",
    "                            text = (\n",
    "                                f\"Form: {filing.get('form_type')}. \"\n",
    "                                f\"Date: {filing.get('filing_date')}. \"\n",
    "                                f\"Accession: {filing.get('accession_no')}. \"\n",
    "                                f\"URL: {filing.get('doc_url')}.\"\n",
    "                            )\n",
    "                            metadata = {\"source\": file_path}\n",
    "                            documents.append({\"text\": text, \"metadata\": metadata})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()\n",
    "if not docs:\n",
    "    print(\"No documents loaded. Check your 'sec_10k_data' directory.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# --- Create FAISS Vector Store ---\n",
    "try:\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    metadatas = [doc[\"metadata\"] for doc in docs]\n",
    "    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating vector store:\", e)\n",
    "\n",
    "# --- Set up the RetrievalQA chain ---\n",
    "try:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever()\n",
    "    )\n",
    "    print(\"RetrievalQA chain is ready.\")\n",
    "except Exception as e:\n",
    "    print(\"Error setting up the QA chain:\", e)\n",
    "\n",
    "# --- Function to compute cosine similarity ---\n",
    "def compute_similarity(answer, documents):\n",
    "    try:\n",
    "        answer_embedding = similarity_model.encode(answer, convert_to_tensor=True)\n",
    "        sims = []\n",
    "        for doc in documents:\n",
    "            doc_embedding = similarity_model.encode(doc, convert_to_tensor=True)\n",
    "            cosine_sim = util.cos_sim(answer_embedding, doc_embedding)\n",
    "            sims.append(cosine_sim.item())\n",
    "        if sims:\n",
    "            avg_sim = sum(sims) / len(sims)\n",
    "            return avg_sim, sims\n",
    "        else:\n",
    "            return 0, []\n",
    "    except Exception as e:\n",
    "        print(\"Error computing similarity:\", e)\n",
    "        return 0, []\n",
    "\n",
    "# --- Interactive Query Loop ---\n",
    "def interactive_query():\n",
    "    print(\"\\n--- RAG Query System ---\")\n",
    "    base_query = input(\"Enter your query about 10-K filings: \")\n",
    "    query = base_query.strip()\n",
    "    while True:\n",
    "        try:\n",
    "            answer = qa_chain.run(query)\n",
    "            print(\"\\nLLM Answer:\\n\", answer)\n",
    "            \n",
    "            # Retrieve top 3 documents for context\n",
    "            retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "            print(\"\\nTop Retrieved Documents:\")\n",
    "            for i, doc in enumerate(retrieved_docs, start=1):\n",
    "                print(f\"\\nDocument {i} (Source: {doc.metadata.get('source', 'N/A')}):\")\n",
    "                print(doc.page_content)\n",
    "            \n",
    "            # Compute cosine similarity for faithfulness metrics\n",
    "            retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "            avg_similarity, sims = compute_similarity(answer, retrieved_texts)\n",
    "            print(\"\\nFaithfulness Metrics:\")\n",
    "            print(\"Average Cosine Similarity:\", avg_similarity)\n",
    "            print(\"Individual Similarities:\", sims)\n",
    "        except Exception as e:\n",
    "            print(\"Error during query processing:\", e)\n",
    "        \n",
    "        refine = input(\"\\nWould you like to refine your query with additional details? (yes/no): \").lower().strip()\n",
    "        if refine in ['yes', 'y']:\n",
    "            extra = input(\"Enter additional details (e.g., specific year, company name, etc.): \").strip()\n",
    "            query = base_query + \" \" + extra\n",
    "        else:\n",
    "            break\n",
    "\n",
    "interactive_query()\n",
    "\n",
    "# Future improvements:\n",
    "# - Fine-tune the LLM on your SEC filings data.\n",
    "# - Optimize query formulation and ask follow-up questions automatically if details are missing.\n",
    "# - Track and display top-searched topics for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the final, merged code. It integrates the following enhancements:\n",
    "\n",
    "• A reranker using a free cross‑encoder for better document selection.\n",
    "\n",
    "• Query decomposition to break complex queries into sub‑questions.\n",
    "\n",
    "• An interactive, React‑style query loop that lets you refine queries with extra details.\n",
    "\n",
    "Run this cell in your notebook to launch the enhanced RAG system using only free tools.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How This Code Works:\n",
    "\n",
    "Auto-installation: It ensures all necessary packages are installed.\n",
    "\n",
    "Vector Store & RetrievalQA: It loads SEC 10-K filing metadata, builds a FAISS vector store, and sets up a retrieval chain using a free LLM (google/flan-t5-small).\n",
    "\n",
    "Reranking: After initial retrieval, it reranks the top documents using a cross-encoder for improved relevance.\n",
    "\n",
    "Query Decomposition: For complex queries, it decomposes the question into sub‑questions.\n",
    "\n",
    "Interactive Loop: You can iteratively refine your query (e.g., add a year or company name) and the system displays the answer, top documents, and faithfulness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nih4r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "C:\\Users\\Nih4r\\AppData\\Local\\Temp\\ipykernel_35348\\3966269194.py:52: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Nih4r\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nih4r\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2746 documents.\n",
      "Vector store created successfully.\n",
      "RetrievalQA chain is ready.\n",
      "\n",
      "--- RAG Query System with Reranking, Decomposition, and Reflection ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "# --- Auto-install required packages ---\n",
    "def install_if_needed(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg_name])\n",
    "\n",
    "# Basic packages for our pipeline\n",
    "packages = [\n",
    "    (\"langchain\", None),\n",
    "    (\"langchain-community\", None),\n",
    "    (\"langchain-huggingface\", \"langchain_huggingface\"),\n",
    "    (\"transformers\", None),\n",
    "    (\"sentence_transformers\", \"sentence_transformers\"),\n",
    "    (\"faiss-cpu\", None)\n",
    "]\n",
    "for pkg, imp in packages:\n",
    "    install_if_needed(pkg, imp)\n",
    "\n",
    "# --- Import libraries ---\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "# Try importing the new HuggingFacePipeline from langchain_huggingface; fallback if needed.\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "except ImportError:\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import transformers\n",
    "\n",
    "# --- Set up free LLM using HuggingFacePipeline ---\n",
    "model_name = \"google/flan-t5-small\"\n",
    "pipe = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    temperature=0\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --- Set up embeddings for vector store and similarity ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Set up cross-encoder for reranking ---\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# --- Function to load SEC 10-K documents from JSON files ---\n",
    "def load_documents(data_dir=\"sec_10k_data\"):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        filings = json.load(f)\n",
    "                        for filing in filings:\n",
    "                            text = (\n",
    "                                f\"Form: {filing.get('form_type')}. \"\n",
    "                                f\"Date: {filing.get('filing_date')}. \"\n",
    "                                f\"Accession: {filing.get('accession_no')}. \"\n",
    "                                f\"URL: {filing.get('doc_url')}.\"\n",
    "                            )\n",
    "                            metadata = {\"source\": file_path}\n",
    "                            documents.append({\"text\": text, \"metadata\": metadata})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()\n",
    "if not docs:\n",
    "    print(\"No documents loaded. Check your 'sec_10k_data' directory.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# --- Create FAISS Vector Store ---\n",
    "try:\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    metadatas = [doc[\"metadata\"] for doc in docs]\n",
    "    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating vector store:\", e)\n",
    "\n",
    "# --- Set up the RetrievalQA chain ---\n",
    "try:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever()\n",
    "    )\n",
    "    print(\"RetrievalQA chain is ready.\")\n",
    "except Exception as e:\n",
    "    print(\"Error setting up the QA chain:\", e)\n",
    "\n",
    "# --- Function to compute cosine similarity ---\n",
    "def compute_similarity(answer, documents):\n",
    "    try:\n",
    "        answer_embedding = similarity_model.encode(answer, convert_to_tensor=True)\n",
    "        sims = []\n",
    "        for doc in documents:\n",
    "            doc_embedding = similarity_model.encode(doc, convert_to_tensor=True)\n",
    "            cosine_sim = util.cos_sim(answer_embedding, doc_embedding)\n",
    "            sims.append(cosine_sim.item())\n",
    "        if sims:\n",
    "            avg_sim = sum(sims) / len(sims)\n",
    "            return avg_sim, sims\n",
    "        else:\n",
    "            return 0, []\n",
    "    except Exception as e:\n",
    "        print(\"Error computing similarity:\", e)\n",
    "        return 0, []\n",
    "\n",
    "# --- Function for reranking retrieved documents ---\n",
    "def rerank_documents(query, docs, top_k=3):\n",
    "    pairs = [(query, doc.page_content) for doc in docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in reranked[:top_k]]\n",
    "\n",
    "# --- Function for query decomposition ---\n",
    "def decompose_query(query):\n",
    "    prompt = f\"Decompose this query into sub-questions to get more detailed answers: '{query}'\"\n",
    "    subqueries = llm(prompt)\n",
    "    return [sq.strip() for sq in subqueries.split(\"\\n\") if sq.strip()]\n",
    "\n",
    "# --- Interactive Query Loop with React Reflection ---\n",
    "def interactive_query():\n",
    "    print(\"\\n--- RAG Query System with Reranking, Decomposition, and Reflection ---\")\n",
    "    base_query = input(\"Enter your query about 10-K filings: \").strip()\n",
    "    query = base_query\n",
    "    while True:\n",
    "        try:\n",
    "            # If query is complex, decompose it into sub-questions.\n",
    "            if len(query.split()) > 10:\n",
    "                sub_queries = decompose_query(query)\n",
    "                print(\"\\nSub-queries generated:\")\n",
    "                for idx, sub in enumerate(sub_queries, 1):\n",
    "                    print(f\"{idx}. {sub}\")\n",
    "            else:\n",
    "                sub_queries = [query]\n",
    "            \n",
    "            # Retrieve a broader set of documents.\n",
    "            initial_docs = vector_store.similarity_search(query, k=10)\n",
    "            if not initial_docs:\n",
    "                print(\"No relevant documents found.\")\n",
    "                break\n",
    "            \n",
    "            # Rerank documents using the cross-encoder.\n",
    "            top_docs = rerank_documents(query, initial_docs, top_k=3)\n",
    "            \n",
    "            # Generate an answer using the RetrievalQA chain.\n",
    "            answer = qa_chain.run(query)\n",
    "            print(\"\\nLLM Answer:\\n\", answer)\n",
    "            \n",
    "            # Display top retrieved documents.\n",
    "            print(\"\\nTop Retrieved Documents after Reranking:\")\n",
    "            for i, doc in enumerate(top_docs, start=1):\n",
    "                print(f\"\\nDocument {i} (Source: {doc.metadata.get('source', 'N/A')}):\")\n",
    "                print(doc.page_content)\n",
    "            \n",
    "            # Compute cosine similarity as a proxy for faithfulness.\n",
    "            retrieved_texts = [doc.page_content for doc in top_docs]\n",
    "            avg_similarity, sims = compute_similarity(answer, retrieved_texts)\n",
    "            print(\"\\nFaithfulness Metrics:\")\n",
    "            print(\"Average Cosine Similarity:\", avg_similarity)\n",
    "            print(\"Individual Similarities:\", sims)\n",
    "        except Exception as e:\n",
    "            print(\"Error during query processing:\", e)\n",
    "        \n",
    "        refine = input(\"\\nWould you like to refine your query with additional details? (yes/no): \").lower().strip()\n",
    "        if refine in ['yes', 'y']:\n",
    "            extra = input(\"Enter additional details (e.g., specific year, company name, etc.): \").strip()\n",
    "            query = base_query + \" \" + extra\n",
    "            print(\"Refining query...\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "interactive_query()\n",
    "\n",
    "# Future improvements:\n",
    "# - Fine-tune the LLM on your SEC filings data.\n",
    "# - Optimize query formulation and ask follow-up questions automatically if details are missing.\n",
    "# - Track and display top-searched topics for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
