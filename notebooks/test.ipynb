{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pandas ...\n",
      "Installing openpyxl ...\n",
      "Installing requests ...\n",
      "\n",
      "Installation completed.\n",
      "All packages installed successfully.\n",
      "Elapsed time: 6.53 seconds.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Upgrade pip first\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error upgrading pip:\", e)\n",
    "\n",
    "# Required packages for this code snippet\n",
    "packages = [\"pandas\", \"openpyxl\", \"requests\"]\n",
    "\n",
    "errors = []\n",
    "start_time = time.time()\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        print(f\"Installing {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        errors.append(f\"{pkg}: {e}\")\n",
    "        print(f\"Failed to install {pkg}. Continuing.\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"\\nInstallation completed.\")\n",
    "if errors:\n",
    "    print(\"Encountered the following errors:\")\n",
    "    for err in errors:\n",
    "        print(\" -\", err)\n",
    "else:\n",
    "    print(\"All packages installed successfully.\")\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK not found for GATO\n",
      "CIK not found for HYZN\n",
      "CIK not found for FFIE\n",
      "CIK not found for FFIEW\n",
      "CIK not found for WESTW\n",
      "CIK not found for PXD\n",
      "CIK not found for CHKEZ\n",
      "CIK not found for TELZ\n",
      "CIK not found for CHKEL\n",
      "CIK not found for NS\n",
      "CIK not found for CEI\n",
      "CIK not found for IVCP\n",
      "CIK not found for NVTA\n",
      "CIK not found for VAXX\n",
      "CIK not found for AGLE\n",
      "CIK not found for B\n",
      "CIK not found for NCR\n",
      "CIK not found for FRG\n",
      "CIK not found for SUNW\n",
      "CIK not found for CTIB\n",
      "CIK not found for HTIA\n",
      "CIK not found for WE\n",
      "CIK not found for MDRRP\n",
      "CIK not found for AYX\n",
      "CIK not found for PAYOW\n",
      "CIK not found for CPSI\n",
      "CIK not found for MVLA\n",
      "CIK not found for GSD\n",
      "CIK not found for TBC\n",
      "CIK not found for HEAR\n",
      "CIK not found for ITI\n",
      "CIK not found for AESC\n",
      "CIK not found for PNM\n",
      "CIK not found for PEGY\n",
      "Script executed successfully.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package])\n",
    "\n",
    "for pkg in [\"pandas\", \"openpyxl\", \"requests\"]:\n",
    "    install_if_needed(pkg)\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Update headers with a proper User-Agent string (replace with your actual contact email)\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"RAGFinancialReport/1.0 (niharskumar@gmail.com)\"\n",
    "}\n",
    "\n",
    "def get_all_tickers_info():\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching tickers info:\", e)\n",
    "        return {}\n",
    "\n",
    "def find_cik(ticker, tickers_data):\n",
    "    try:\n",
    "        for key, info in tickers_data.items():\n",
    "            if info[\"ticker\"].lower() == ticker.lower():\n",
    "                return info[\"cik_str\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error finding CIK for\", ticker, \":\", e)\n",
    "    return None\n",
    "\n",
    "def fetch_10k(cik, year):\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{str(cik).zfill(10)}.json\"\n",
    "    try:\n",
    "        r = requests.get(base_url, headers=HEADERS, timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to fetch data for CIK {cik} in {year}: status code {r.status_code}\")\n",
    "            return None\n",
    "        data = r.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for CIK {cik} in {year}:\", e)\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        recent = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        forms = recent.get(\"form\", [])\n",
    "        dates = recent.get(\"filingDate\", [])\n",
    "        accessions = recent.get(\"accessionNumber\", [])\n",
    "\n",
    "        for i in range(len(forms)):\n",
    "            form_type = forms[i]\n",
    "            filing_date = dates[i]\n",
    "            accession_no = accessions[i]\n",
    "            if form_type == \"10-K\" and filing_date.startswith(str(year)):\n",
    "                doc_url = (\n",
    "                    f\"https://www.sec.gov/Archives/edgar/data/\"\n",
    "                    f\"{int(cik)}/{accession_no.replace('-', '')}/{accession_no}-index.htm\"\n",
    "                )\n",
    "                results.append({\n",
    "                    \"form_type\": form_type,\n",
    "                    \"filing_date\": filing_date,\n",
    "                    \"accession_no\": accession_no,\n",
    "                    \"doc_url\": doc_url\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing filings for CIK {cik} in {year}:\", e)\n",
    "        return None\n",
    "\n",
    "    return results if results else None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        excel_file = r\"C:\\Users\\Nih4r\\Documents\\GitHub\\RAG_Financial_Report\\notebooks\\data\\Data_companies_list.xlsx\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_file)\n",
    "        except Exception as e:\n",
    "            print(\"Error reading Excel file:\", excel_file, e)\n",
    "            return\n",
    "\n",
    "        tickers_data = get_all_tickers_info()\n",
    "        if not tickers_data:\n",
    "            print(\"No tickers data available. Exiting.\")\n",
    "            return\n",
    "\n",
    "        years = range(2012, 2026)\n",
    "        output_dir = \"sec_10k_data\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            ticker = str(row[\"Symbol\"]).strip()\n",
    "            cik = find_cik(ticker, tickers_data)\n",
    "            if not cik:\n",
    "                print(f\"CIK not found for {ticker}\")\n",
    "                continue\n",
    "\n",
    "            ticker_dir = os.path.join(output_dir, ticker)\n",
    "            os.makedirs(ticker_dir, exist_ok=True)\n",
    "\n",
    "            for y in years:\n",
    "                filings = fetch_10k(cik, y)\n",
    "                if filings:\n",
    "                    save_path = os.path.join(ticker_dir, f\"{ticker}_{y}_10K.json\")\n",
    "                    try:\n",
    "                        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(filings, f, indent=2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving file {save_path}:\", e)\n",
    "                sleep(0.2)\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error in main execution:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Script executed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2746 documents.\n",
      "Vector store created successfully.\n",
      "RetrievalQA chain is ready.\n",
      "\n",
      "--- RAG Query System ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "# --- Auto-install required packages ---\n",
    "def install_if_needed(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg_name])\n",
    "\n",
    "# Basic packages for our pipeline\n",
    "for pkg, imp in [(\"langchain\", None), (\"langchain-community\", None),\n",
    "                 (\"langchain-huggingface\", \"langchain_huggingface\"),\n",
    "                 (\"transformers\", None), (\"sentence_transformers\", \"sentence_transformers\"),\n",
    "                 (\"faiss-cpu\", None)]:\n",
    "    install_if_needed(pkg, imp)\n",
    "\n",
    "# --- Import libraries ---\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "# Try importing the new HuggingFacePipeline from langchain_huggingface, fallback if needed.\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "except ImportError:\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import transformers\n",
    "\n",
    "# --- Set up free LLM using HuggingFacePipeline ---\n",
    "model_name = \"google/flan-t5-small\"\n",
    "# Enable sampling to use temperature (even if 0 yields greedy behavior)\n",
    "pipe = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    temperature=0\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --- Set up embeddings for vector store and similarity ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Function to load SEC 10-K documents from JSON files ---\n",
    "def load_documents(data_dir=\"sec_10k_data\"):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        filings = json.load(f)\n",
    "                        for filing in filings:\n",
    "                            text = (\n",
    "                                f\"Form: {filing.get('form_type')}. \"\n",
    "                                f\"Date: {filing.get('filing_date')}. \"\n",
    "                                f\"Accession: {filing.get('accession_no')}. \"\n",
    "                                f\"URL: {filing.get('doc_url')}.\"\n",
    "                            )\n",
    "                            metadata = {\"source\": file_path}\n",
    "                            documents.append({\"text\": text, \"metadata\": metadata})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()\n",
    "if not docs:\n",
    "    print(\"No documents loaded. Check your 'sec_10k_data' directory.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# --- Create FAISS Vector Store ---\n",
    "try:\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    metadatas = [doc[\"metadata\"] for doc in docs]\n",
    "    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating vector store:\", e)\n",
    "\n",
    "# --- Set up the RetrievalQA chain ---\n",
    "try:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever()\n",
    "    )\n",
    "    print(\"RetrievalQA chain is ready.\")\n",
    "except Exception as e:\n",
    "    print(\"Error setting up the QA chain:\", e)\n",
    "\n",
    "# --- Function to compute cosine similarity ---\n",
    "def compute_similarity(answer, documents):\n",
    "    try:\n",
    "        answer_embedding = similarity_model.encode(answer, convert_to_tensor=True)\n",
    "        sims = []\n",
    "        for doc in documents:\n",
    "            doc_embedding = similarity_model.encode(doc, convert_to_tensor=True)\n",
    "            cosine_sim = util.cos_sim(answer_embedding, doc_embedding)\n",
    "            sims.append(cosine_sim.item())\n",
    "        if sims:\n",
    "            avg_sim = sum(sims) / len(sims)\n",
    "            return avg_sim, sims\n",
    "        else:\n",
    "            return 0, []\n",
    "    except Exception as e:\n",
    "        print(\"Error computing similarity:\", e)\n",
    "        return 0, []\n",
    "\n",
    "# --- Interactive Query Loop ---\n",
    "def interactive_query():\n",
    "    print(\"\\n--- RAG Query System ---\")\n",
    "    base_query = input(\"Enter your query about 10-K filings: \")\n",
    "    query = base_query.strip()\n",
    "    while True:\n",
    "        try:\n",
    "            answer = qa_chain.run(query)\n",
    "            print(\"\\nLLM Answer:\\n\", answer)\n",
    "            \n",
    "            # Retrieve top 3 documents for context\n",
    "            retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "            print(\"\\nTop Retrieved Documents:\")\n",
    "            for i, doc in enumerate(retrieved_docs, start=1):\n",
    "                print(f\"\\nDocument {i} (Source: {doc.metadata.get('source', 'N/A')}):\")\n",
    "                print(doc.page_content)\n",
    "            \n",
    "            # Compute cosine similarity for faithfulness metrics\n",
    "            retrieved_texts = [doc.page_content for doc in retrieved_docs]\n",
    "            avg_similarity, sims = compute_similarity(answer, retrieved_texts)\n",
    "            print(\"\\nFaithfulness Metrics:\")\n",
    "            print(\"Average Cosine Similarity:\", avg_similarity)\n",
    "            print(\"Individual Similarities:\", sims)\n",
    "        except Exception as e:\n",
    "            print(\"Error during query processing:\", e)\n",
    "        \n",
    "        refine = input(\"\\nWould you like to refine your query with additional details? (yes/no): \").lower().strip()\n",
    "        if refine in ['yes', 'y']:\n",
    "            extra = input(\"Enter additional details (e.g., specific year, company name, etc.): \").strip()\n",
    "            query = base_query + \" \" + extra\n",
    "        else:\n",
    "            break\n",
    "\n",
    "interactive_query()\n",
    "\n",
    "# Future improvements:\n",
    "# - Fine-tune the LLM on your SEC filings data.\n",
    "# - Optimize query formulation and ask follow-up questions automatically if details are missing.\n",
    "# - Track and display top-searched topics for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the final, merged code. It integrates the following enhancements:\n",
    "\n",
    "• A reranker using a free cross‑encoder for better document selection.\n",
    "\n",
    "• Query decomposition to break complex queries into sub‑questions.\n",
    "\n",
    "• An interactive, React‑style query loop that lets you refine queries with extra details.\n",
    "\n",
    "Run this cell in your notebook to launch the enhanced RAG system using only free tools.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "How This Code Works:\n",
    "\n",
    "Auto-installation: It ensures all necessary packages are installed.\n",
    "\n",
    "Vector Store & RetrievalQA: It loads SEC 10-K filing metadata, builds a FAISS vector store, and sets up a retrieval chain using a free LLM (google/flan-t5-small).\n",
    "\n",
    "Reranking: After initial retrieval, it reranks the top documents using a cross-encoder for improved relevance.\n",
    "\n",
    "Query Decomposition: For complex queries, it decomposes the question into sub‑questions.\n",
    "\n",
    "Interactive Loop: You can iteratively refine your query (e.g., add a year or company name) and the system displays the answer, top documents, and faithfulness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "# --- Auto-install required packages ---\n",
    "def install_if_needed(pkg_name, import_name=None):\n",
    "    try:\n",
    "        __import__(import_name or pkg_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", pkg_name])\n",
    "\n",
    "# Basic packages for our pipeline\n",
    "packages = [\n",
    "    (\"langchain\", None),\n",
    "    (\"langchain-community\", None),\n",
    "    (\"langchain-huggingface\", \"langchain_huggingface\"),\n",
    "    (\"transformers\", None),\n",
    "    (\"sentence_transformers\", \"sentence_transformers\"),\n",
    "    (\"faiss-cpu\", None)\n",
    "]\n",
    "for pkg, imp in packages:\n",
    "    install_if_needed(pkg, imp)\n",
    "\n",
    "# --- Import libraries ---\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "# Try importing the new HuggingFacePipeline from langchain_huggingface; fallback if needed.\n",
    "try:\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "except ImportError:\n",
    "    from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import transformers\n",
    "\n",
    "# --- Set up free LLM using HuggingFacePipeline ---\n",
    "model_name = \"google/flan-t5-small\"\n",
    "pipe = transformers.pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    temperature=0\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --- Set up embeddings for vector store and similarity ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Set up cross-encoder for reranking ---\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# --- Function to load SEC 10-K documents from JSON files ---\n",
    "def load_documents(data_dir=\"sec_10k_data\"):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        filings = json.load(f)\n",
    "                        for filing in filings:\n",
    "                            text = (\n",
    "                                f\"Form: {filing.get('form_type')}. \"\n",
    "                                f\"Date: {filing.get('filing_date')}. \"\n",
    "                                f\"Accession: {filing.get('accession_no')}. \"\n",
    "                                f\"URL: {filing.get('doc_url')}.\"\n",
    "                            )\n",
    "                            metadata = {\"source\": file_path}\n",
    "                            documents.append({\"text\": text, \"metadata\": metadata})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "docs = load_documents()\n",
    "if not docs:\n",
    "    print(\"No documents loaded. Check your 'sec_10k_data' directory.\")\n",
    "else:\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "\n",
    "# --- Create FAISS Vector Store ---\n",
    "try:\n",
    "    texts = [doc[\"text\"] for doc in docs]\n",
    "    metadatas = [doc[\"metadata\"] for doc in docs]\n",
    "    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating vector store:\", e)\n",
    "\n",
    "# --- Set up the RetrievalQA chain ---\n",
    "try:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever()\n",
    "    )\n",
    "    print(\"RetrievalQA chain is ready.\")\n",
    "except Exception as e:\n",
    "    print(\"Error setting up the QA chain:\", e)\n",
    "\n",
    "# --- Function to compute cosine similarity ---\n",
    "def compute_similarity(answer, documents):\n",
    "    try:\n",
    "        answer_embedding = similarity_model.encode(answer, convert_to_tensor=True)\n",
    "        sims = []\n",
    "        for doc in documents:\n",
    "            doc_embedding = similarity_model.encode(doc, convert_to_tensor=True)\n",
    "            cosine_sim = util.cos_sim(answer_embedding, doc_embedding)\n",
    "            sims.append(cosine_sim.item())\n",
    "        if sims:\n",
    "            avg_sim = sum(sims) / len(sims)\n",
    "            return avg_sim, sims\n",
    "        else:\n",
    "            return 0, []\n",
    "    except Exception as e:\n",
    "        print(\"Error computing similarity:\", e)\n",
    "        return 0, []\n",
    "\n",
    "# --- Function for reranking retrieved documents ---\n",
    "def rerank_documents(query, docs, top_k=3):\n",
    "    pairs = [(query, doc.page_content) for doc in docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, score in reranked[:top_k]]\n",
    "\n",
    "# --- Function for query decomposition ---\n",
    "def decompose_query(query):\n",
    "    prompt = f\"Decompose this query into sub-questions to get more detailed answers: '{query}'\"\n",
    "    subqueries = llm(prompt)\n",
    "    return [sq.strip() for sq in subqueries.split(\"\\n\") if sq.strip()]\n",
    "\n",
    "# --- Interactive Query Loop with React Reflection ---\n",
    "def interactive_query():\n",
    "    print(\"\\n--- RAG Query System with Reranking, Decomposition, and Reflection ---\")\n",
    "    base_query = input(\"Enter your query about 10-K filings: \").strip()\n",
    "    query = base_query\n",
    "    while True:\n",
    "        try:\n",
    "            # If query is complex, decompose it into sub-questions.\n",
    "            if len(query.split()) > 10:\n",
    "                sub_queries = decompose_query(query)\n",
    "                print(\"\\nSub-queries generated:\")\n",
    "                for idx, sub in enumerate(sub_queries, 1):\n",
    "                    print(f\"{idx}. {sub}\")\n",
    "            else:\n",
    "                sub_queries = [query]\n",
    "            \n",
    "            # Retrieve a broader set of documents.\n",
    "            initial_docs = vector_store.similarity_search(query, k=10)\n",
    "            if not initial_docs:\n",
    "                print(\"No relevant documents found.\")\n",
    "                break\n",
    "            \n",
    "            # Rerank documents using the cross-encoder.\n",
    "            top_docs = rerank_documents(query, initial_docs, top_k=3)\n",
    "            \n",
    "            # Generate an answer using the RetrievalQA chain.\n",
    "            answer = qa_chain.run(query)\n",
    "            print(\"\\nLLM Answer:\\n\", answer)\n",
    "            \n",
    "            # Display top retrieved documents.\n",
    "            print(\"\\nTop Retrieved Documents after Reranking:\")\n",
    "            for i, doc in enumerate(top_docs, start=1):\n",
    "                print(f\"\\nDocument {i} (Source: {doc.metadata.get('source', 'N/A')}):\")\n",
    "                print(doc.page_content)\n",
    "            \n",
    "            # Compute cosine similarity as a proxy for faithfulness.\n",
    "            retrieved_texts = [doc.page_content for doc in top_docs]\n",
    "            avg_similarity, sims = compute_similarity(answer, retrieved_texts)\n",
    "            print(\"\\nFaithfulness Metrics:\")\n",
    "            print(\"Average Cosine Similarity:\", avg_similarity)\n",
    "            print(\"Individual Similarities:\", sims)\n",
    "        except Exception as e:\n",
    "            print(\"Error during query processing:\", e)\n",
    "        \n",
    "        refine = input(\"\\nWould you like to refine your query with additional details? (yes/no): \").lower().strip()\n",
    "        if refine in ['yes', 'y']:\n",
    "            extra = input(\"Enter additional details (e.g., specific year, company name, etc.): \").strip()\n",
    "            query = base_query + \" \" + extra\n",
    "            print(\"Refining query...\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "interactive_query()\n",
    "\n",
    "# Future improvements:\n",
    "# - Fine-tune the LLM on your SEC filings data.\n",
    "# - Optimize query formulation and ask follow-up questions automatically if details are missing.\n",
    "# - Track and display top-searched topics for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
